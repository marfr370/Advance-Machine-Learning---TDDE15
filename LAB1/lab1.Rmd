---
title: "TDDE15 - Lab 1"
author: Martin Friberg - marfr370
date: "2021/09/10"
output:
  pdf_document:
    fig_caption: true
    number_sections: true
    latex_engine: xelatex

  html_document:
    df_print: paged
header-includes:
  - \usepackage{caption}
  - \usepackage{float}
  - \floatplacement{figure}{H}
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
```

# Assignment 1 {-}

Show that multiple runs of the hill-climbing algorithm can return non-equivalent Bayesian network (BN) structures.

Two DAGs represent the same independencies according to the separation criterion (i.e. they are equivalent) if and only if they have the same adjacencies and unshielded colliders. The causal variables influencing the collider are themselves not necessarily associated. If they are not adjacent, the collider is unshielded. 

```{r , results='hide', message=FALSE, warning=FALSE}

library(bnlearn)
library(RBGL)
library(Rgraphviz)
library(gRain)

```

```{r ass1, cashe=TRUE}

data("asia")
nodes = names(asia)
e = empty.graph(nodes)


hc_restart0 = hc(asia, restart=0, score="bde", iss=15) 
hc_restart100 = hc(asia, restart=100, score="bde", iss=15) 

information <- function(hc1, hc2){
  #print(arcs(hc1))
  #print(arcs(hc2))
  print('VSTRUCTS 0 RESTARTS')
  #moral = false is the default value and it makes vstructs return unshielded colliders
  print(vstructs(hc1, moral=FALSE))
  print('VSTRUCTS 100 RESTARTS')
  print(vstructs(hc2, moral=FALSE))
  #print('CPDAG:')
  #cpdag(hc1)
  #cpdag(hc2)
  all.equal(hc1,hc2)
}

information(hc_restart0, hc_restart100)
library(Rgraphviz)

``` 

## Explain why this happens {-}
 Since the hill climbing algorithm uses a randomized search approach and is not asymptotically correct under faithfulness it may get stuck in a local maxima where the solution can not be approved upon by neighboring states since the cost function is higher in those. To attempt to avoid getting stuck in local optima, one could use restarts (i.e. repeated local search) which is what we define in the hc algorithm. 
 
 

The variable iss is the user-defined imaginary sample size (the higher the less regularization). 

Bayesian score favours models that trade off fit of data and model complexity.


```{r hcComparison, echo = FALSE, out.height="50%", message=FALSE, fig.cap='Comparison between DAGs generated by HC for different restarts and iss=15', fig.show="hold", out.width="50%"}
graphviz.compare(hc_restart0, hc_restart100, layout = "dot", shape = "circle", main = NULL,
                sub = NULL, diff = "from-first", diff.args = list())
```

As can be seen from the graphs, a higher number of restarts generates a non-equivalent graph compared to the graph generated by the hc-algorithm with 0 restarts, which is due to the before mentioned facts. The graphs are non-equivalent due to the fact that they have different unshielded colliders as well as different adjacencies. 

# Assignment 2 {-}

Learn a BN from 80 % of the Asia dataset. Learn both the structure and the parameters. Use the BN learned to classify the remaining 20 % of the Asia dataset in two classes: S = yes and S = no. To do so, you have to use exact or approximate inference. Compare your results with those of the true Asia BN. 

```{r ass2, cashe=TRUE, fig.width=3, fig.height=2, fig.align="center"}

data('asia')
n=dim(asia)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.8))
data=asia[id,]
test = asia[-id,]


hc_restart100 = hc(data, restart=100, score="bde", iss=5)

true_dag = model2network("[A][S][T|A][L|S][B|S][D|B:E][E|T:L][X|E]")

information(hc_restart100, true_dag)


```

```{r truedag, echo = FALSE, out.height="50%", message=FALSE, fig.cap='Comparison between DAG generated by hc and the true DAG', fig.show="hold", out.width="50%"}
graphviz.compare(hc_restart100, true_dag, layout = "dot", shape = "circle", main = NULL,
                 sub = NULL, diff = "from-first", diff.args = list())
```

```{r ass22, cashe=TRUE, echo=FALSE, warning=FALSE, fig.width=3, fig.height=2, fig.align="center"}

graphviz.plot(true_dag)

bn_fitted = bn.fit(hc_restart100, data = data)
true_bn_fitted = bn.fit(true_dag, data= data)

grain_obj = as.grain(bn_fitted)
true_grain_obj = as.grain(true_bn_fitted)

juncTree = compile(grain_obj)
true_juncTree = compile(true_grain_obj)

```

```{r ass221, echo=TRUE}
summary(juncTree)

```

```{r junctree, echo = FALSE, out.height="50%", message=FALSE, fig.cap='The junction tree created from the DAG through the junction tree algorithm. The junction tree algorithm turns the graph into a tree of clusters where each cluster is connected to a factor from the VE process.', fig.show="hold", out.width="50%", fig.align="center"}
plot(juncTree)
```

```{r ass222, cashe=TRUE, fig.width=3, fig.height=2, fig.align="center"}

predictfunc <- function(testdata, tree, xvars){
  prediction = c()
  for (i in 1:dim(testdata)[1]){
    xvals <- testdata[i, xvars]
    xvals <- as.vector(unlist(xvals, use.names = FALSE))
    states <- setEvidence(tree, nodes = xvars, states = xvals)
    probs <- querygrain(states, nodes="S")
    prediction[i] <- names(which.max(probs$S))
    
  }
  return(prediction)
}

missclass <- function(true, pred){
  1 - sum(diag(table(true, pred)))/sum(table(true, pred))
}

xvars <- colnames(test[, -which(names(test) == "S")])
pred <- predictfunc(test, juncTree, xvars)
asia_Pred <- predictfunc(test, true_juncTree, xvars)
true_s <- test$S
table(true_s, pred)
table(true_s, asia_Pred)
table(pred, asia_Pred)


missclass(true_s, pred)
missclass(true_s, asia_Pred)
```

As can be seen from the confusion matrices, predicting S based on exact inference from the DAG generated through the HC algorithm and from the true DAG generates the exact same results. This is due to the fact that the markov blanket of S is the same for both of the DAGs. More on this in assignment 3. The prediction of S is not optimal, with a misclassification of 0.297. 

# Assignment 3  {-}

Classify S given observations only for the so-called Markov blanket of S, i.e. its parents plus its children plus the parents of its children minus S itself. Report again the confusion matrix.


A subset that contains all the useful information is called a Markov blanket. Since S has no children and therefore no parents of it's children, the Markov blanket only consists of S's parents themselves. In an undirected graph (markov network) the markov blanket is all of the variables that are connected to the variable in question via an edge. 

All of the variables not in the markov blanket of S are independent of S and does therefore not influence S in any way. The forementioned fact leads to that S can be as well predicted through only the Markov Blanket as through the whole DAG. 

```{r ass3, cashe=TRUE, fig.width=3, fig.height=2, fig.align="center"}
graphviz.plot(bn_fitted)

mb_nodes = mb(x=bn_fitted, node='S')
xvars <- colnames(test[, mb_nodes])
xvars

mb_pred <- predictfunc(test, juncTree, xvars)
table(true_s, pred)

missclass(true_s, pred)
```

The forementioned statements are strengthened by the fact that the resulting confusion matrix as well as the misclassification rate is the same when only using the markov blanket as when using all of the variables in the DAG. 


# Assignment 4  {-}
Classification through naïve bayes is done by applying Bayes rule to compute the probability of C given the particular instance of variables x1,...,xn, and from that predicting the class with the highest posterior probability. This computation can be done by making the independence assumption that all the attributes xi are conditionally independent given the value of the class C. For the Naive Bayesian Network all features are considered as attributes and are independent given the class.

```{r ass4, cashe=TRUE, fig.width=3, fig.height=2, fig.align="center"}
naive_dag = model2network("[S][A|S][B|S][T|S][L|S][E|S][X|S][D|S]")
graphviz.plot(naive_dag, main="Naïve Bayes Classifier")
naive_fitted = bn.fit(naive_dag, data = data)
naiveGrain_obj = as.grain(naive_fitted)
naiveJuncTree = compile(naiveGrain_obj)
xvars <- colnames(test[, -which(names(test) == "S")])
naive_prediction <- predictfunc(test, naiveJuncTree, xvars)
table(true_s, naive_prediction)

missclass(true_s, naive_prediction)
```

The resulting confusion matrix from the Naïve Bayes Classifier is a bit worse off than the confusion matrices where the Markov Blanket could be used. For the Naïve Bayes classifier, the misclassification rate is 0.334 compared to the misclassification rate of 0.297 for the markov blanket. The Naïve Bayes Classifier is most often used when our data set is small, i.e we have a small amount of observations since no good probabilistic inference can be done in that case. In our case with the Asia data set that contains 5000 observations, reasonably good probabilistic inference is attained, which trumphs the Naïve Bayes Classifier. 

# Assignment 5 {-}
Explain why you obtain the same or different results in the exercises (2-4).

The explanation to this assignment is described throughout assignments 2-4.     



